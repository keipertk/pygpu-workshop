{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "This notebook presents libraries and tools for making Python code run on\n",
    "NVIDIA GPUs!! The path to running on GPUs goes through improving CPU\n",
    "performance but the target is to run on GPUs.\n",
    "\n",
    "that are focused on making Python\n",
    "code run faster and run on GPUs. Some of it can be run on a CPU\n",
    "but a fair amount of it will need to be run on an NVIDIA GPU.\n",
    "\n",
    "The notebook was tested on the Anaconda Python distribution for Python 3.x.\n",
    "This container should have all of the package dependencies.\n",
    "\n",
    "This notebook follows the accompanying slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example with Numba'></a>\n",
    "# 1. Simple Python Example Using Numba - CPU and GPU\n",
    "\n",
    "\n",
    "This is a simple example that adds two vectors to create a third vector.\n",
    "The computational work is done in a function. To create enough computational\n",
    "work, the vector length is quite large (1,000,000,000).\n",
    "These are single precision vectors.\n",
    "\n",
    "The example starts with a simple Python example with no compiling.\n",
    "Just in case you are interested a simple timing of the addition is\n",
    "printed at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example'></a>\n",
    "## 1.1 Starting Point - Simple Python Code\n",
    "\n",
    "The code below is the baseline Python code (the starting point). It simply\n",
    "adds the elements of two vectors together and stores that in a third vector.\n",
    "The arrays are created using NumPy. The time it takes to perform the addition\n",
    "is measured using the \"time\" module in Python.\n",
    "\n",
    "Based on code from: https://devblogs.nvidia.com/numba-python-cuda-acceleration/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter()\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter()\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example - CPU'></a>\n",
    "## 1.2 Simple Python - CPU Examples\n",
    "\n",
    "The following examples present code that uses Numba to compile the <tt>Add</tt>\n",
    "function in the code. Various options are presented for both CPU compiling\n",
    "and compiling for the NVIDIA GPU. These options use various decorators and\n",
    "options. The discussion to accompany these examples are in the slide deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example single core'></a>\n",
    "### 1.2.1 <tt>@jit</tt> with default target (single CPU core)\n",
    "\n",
    "This example uses the <tt>@jit</tt> decorator in Numba. By default this targets a\n",
    "single core on the CPU.\n",
    "\n",
    "You might try running the cell a few times to get an idea of the average\n",
    "wall clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example multi-core'></a>\n",
    "### 1.2.2 <tt>@jit</tt> with multi-core target and no Python fallback\n",
    "\n",
    "This targets multiple CPU cores using the  <tt>parallel=True</tt>  option in  <tt>@jit</tt>.\n",
    "\n",
    "I also use the option  <tt>nopython=True</tt>  so I can catch my code\n",
    "mistakes. It is a default option, but I like to specify so I know exactly\n",
    "what options I'm using. This option disables any Python fall-back in case\n",
    "Numba cannot compile the code.\n",
    "\n",
    "You might try running the cell a few times to get an idea of the average\n",
    "wall clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True) \n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note</strong>: Currently, we only measure the time for the compiled\n",
    "code. We don't measure\n",
    "the time that includes the compile time. Therefore, the elapsed wall clock\n",
    "times should not vary too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example defaults cache'></a>\n",
    "### 1.2.3 <tt>@jit</tt> with defaults, caching, and no Python fallback\n",
    "\n",
    "This example uses the defaults again, but it also caches the compiled code\n",
    "so it can be reused (<tt>cache=True</tt>). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, cache=True) \n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example multicore cache'></a>\n",
    "### 1.2.4 <tt>@jit</tt> with parallel, caching, and no Python fallback\n",
    "\n",
    "This example adds the option <tt>parallel=True</tt> (multi-core) to the\n",
    "previous options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True, cache=True) \n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example vectorize no type signature single core'></a>\n",
    "### 1.2.5 <tt>@vectorize</tt> decorator for CPUs (no type signature)\n",
    "\n",
    "This example uses the @vectorize decorator targeting the CPU (default target). \n",
    "Remember that the <tt>@vectorize</tt> decorator compiles functions that perform\n",
    "element-by-element operations (that is, the same operation to every element\n",
    "in the array). The code will most likely contain loops.\n",
    "\n",
    "In general, you give the <tt>@vectorize</tt> decorator a type signature that\n",
    "tells Numba how to build the compiled code for a specific data type (you can\n",
    "specify more than one which is really cool). Here is an example,\n",
    "\n",
    "\n",
    "<codeblock>\n",
    "    @vectorize(['float32(float32, float32)'])\n",
    "</codeblock>\n",
    "\n",
    "\n",
    "The data type specification after the decorator is the \"type signature\".\n",
    "\n",
    "You don't have to specify a type signature. If you don't, then Numba will\n",
    "create a dynamic universal function (DUFunc). This dynamically compilers a\n",
    "new kernel when the function is called with a data type that wasn't previously\n",
    "used. That is, it will compile the function for every specific data type in\n",
    "your code. This can have an impact if your use multiple data types with the same\n",
    "function. You can think of this approach as \"call-time\" or \"lazy\" compilation.\n",
    "\n",
    "Below is an example of using the <tt>@vectorize</tt> decorator without a type\n",
    "signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example vectorize type signature single core'></a>\n",
    "### 1.2.6 @vectorize decorator for CPUs (type signature)\n",
    "\n",
    "Below is an example of using the @vectorize decorator with \"type signature(s)\".\n",
    "Only a single type signature is defined, but you can\n",
    "provide several \"type signatures\", each with different data types if you\n",
    "wished (notice that the type signature is an list (array) ). You would probably\n",
    "only do this if you used the same function with different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'])\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example vectorize type signature single core'></a>\n",
    "### 1.2.7 @vectorize decorator for CPUs with single CPU core target\n",
    "\n",
    "This is a trivial example, but it illustrates that the default target for the\n",
    "<tt>@vectorize</tt> decorator is a single core ( <tt>target='cpu'</tt> ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cpu')\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example vectorize type signature parallel'></a>\n",
    "### 1.2.8 @vectorize decorator using all CPU cores\n",
    "\n",
    "This example changes the target to use all of the cores (multi-core). The target name\n",
    "is simple <tt>parallel</tt>. Even though all of the cores are being used, the performance\n",
    "may not improve. It takes time to move data around as needed across the cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='parallel')\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example loops'></a>\n",
    "### 1.2.9 Using @jit with loops in the function\n",
    "\n",
    "The simple Python code uses array functions. What should you do if the code\n",
    "uses loops? The next cell illustrate how to write the appropriate\n",
    "Python code for Numba. It also shows how to use the  <tt>@jit</tt>  decorator\n",
    "on CPUs.\n",
    "\n",
    "The first cell uses the  <tt>@jit</tt>  decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the @jit decorator\n",
    "\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def Add(a, b, c):\n",
    "    max_length = len(a)\n",
    "    for i in range(0, max_length):\n",
    "        c[i] = a[i] + b[i]\n",
    "    # end for\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on CPU\n",
    "start_time = perf_counter( )\n",
    "Add(A, B, C)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example - GPU'></a>\n",
    "## 1.3 Simple Python - GPU Examples\n",
    "\n",
    "The examples below explore using the GPU as a target with the vectorize decorator\n",
    "and the <tt>cuda</tt> target, as well as using the  <tt>@cuda.jit</tt>  decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example - Vectorize GPU'></a>\n",
    "### 1.3.1 <tt>@vectorize</tt> Decorator with a CUDA Target\n",
    "\n",
    "Porting Python code to the GPU can be very easy using the code from the  <tt>@vectorize</tt>\n",
    "decorator. For most code, all you need to do is change the *target* in the decorator to\n",
    "<tt>cuda</tt>.\n",
    "\n",
    "Notice how Numba takes care of copying the data to the GPU and copying it back. Numba\n",
    "also takes care of defining the arrays on the GPU.\n",
    "\n",
    "Try running the cell several times to get an idea of the compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def Add(a, b):\n",
    "    return a + b\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on GPU\n",
    "start_time = perf_counter( )\n",
    "C = Add(A, B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Simple Python Example - cud.jit GPU'></a>\n",
    "### 1.3.2 <tt>@cuda.jit</tt> Decorator\n",
    "\n",
    "You can also use a different decorator,  <tt>@cuda.jit</tt>, that allows you to\n",
    "write Python code that is more CUDA-like. This offers\n",
    "you greater flexibility and control over your code and possibly better performance.\n",
    "However, you will need to know a fair amount about CUDA  before using it.\n",
    "The link below is a good place to get more information.\n",
    "\n",
    "\n",
    "http://numba.pydata.org/numba-doc/dev/cuda/index.html\n",
    "\n",
    "    \n",
    "As explained in the slides, using the  <tt>cuda.jit</tt>  decorator requires some extra coding.\n",
    "You have to explicitly write the loops in your code.\n",
    "Also, everything you pass into or out-of the compiled function, has to be a NumPy\n",
    "array (even scalars which are NumPy arrays of size 1). However, you can define simple\n",
    "scalars in the function that are not NumPy arrays. For example, loop counters.\n",
    "\n",
    "Pay close attention to how the data is passed to the function - it is much more\n",
    "like functions in C where everything all data is passed through the function call. \n",
    "\n",
    "To prepare for using <tt>@cuda.jit</tt> the cell below rewrites the Python code into\n",
    "loops and uses the <tt>@cuda.jit<tt> decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def Add(a, b, c):\n",
    "    max_length = len(a)\n",
    "    for i in range(0, max_length):\n",
    "        c[i] = a[i] + b[i]\n",
    "    # end for\n",
    "# end def\n",
    "\n",
    "# Initialize arrays\n",
    "N = 100000\n",
    "A = np.ones(N, dtype=np.float32)\n",
    "B = np.ones(A.shape, dtype=A.dtype)\n",
    "C = np.empty_like(A, dtype=A.dtype)\n",
    "\n",
    "# Add arrays on GPU\n",
    "start_time = perf_counter( )\n",
    "Add(A, B, C)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(C)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example'></a>\n",
    "# 2. Python Porting Example\n",
    "\n",
    "This is an example of porting functions to use Numba. It is really an example of the porting\n",
    "<em>lifecycle</em> of a Python application. It starts with a serial Python code\n",
    "that has been written to test the idea and to make sure it works and gets correct answers.\n",
    "\n",
    "Then it moves to putting computational intensive portions of the code into functions.\n",
    "\n",
    "Then it uses Numba to compile for the CPU, using the <tt>@jit</tt> decorator. \n",
    "\n",
    "The next step is to switch to the <tt>@vectorize</tt> decorator, targeting a single core\n",
    "(target is <tt>cpu</tt>), then all of the CPU cores (target is <tt>parallel</tt>), and\n",
    "finally to NVIDIA GPUs (target is <tt>cuda</tt>).\n",
    "\n",
    "The last step is to modify the code to use the <tt>@cuda.jit</tt> decorator. This\n",
    "requires some code changes.\n",
    "\n",
    "The new example code is a very, very simplified version of the start of a Molecular\n",
    "Dynamic (MD) mini-app. It focuses on loops that are initialized (the values are arbitrary\n",
    "and don't mean anything). The code an obviously be made much simpler and faster, but that is\n",
    "not the point. The point is to show you how start with a code that uses loops and \"port\"\n",
    "it to use Numba and GPUs. Several steps will be used in this porting process. Hopefully it\n",
    "gives you some \"feel\" in porting your applications to use Numba.\n",
    "                                                    \n",
    "Let's start with the serial Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# main loop\n",
    "start_time = perf_counter( )\n",
    "d_num = 500\n",
    "p_num = 500\n",
    "\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "for j in range(0, p_num):\n",
    "    for i in range(0, d_num):\n",
    "        pos[i,j] = 6.5\n",
    "    # end for i\n",
    "    for i in range(0, d_num):\n",
    "        accel[i,j] = 4.2*pos[i,j]\n",
    "    # end for\n",
    "# end for j\n",
    "\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Non-Numba Python Code - Function'></a>\n",
    "## 2.1 Non-Numba Python Code\n",
    "\n",
    "The next code creates a function for the computationally intense part of the code (the loop). \n",
    "This gets the code ready for Numba (Remember that Numba compiles functions, not entire codes).\n",
    "Notice that the two arrays are created in the function and are returned to the calling function\n",
    "(both arrays are returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always be sure to check that you answers are correct after you take another\n",
    "step in porting it. Checking answers for this simple code is fairly easy\n",
    "and can be done by simply printing the arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example jit decorator single'></a>\n",
    "## 2.2 <tt>@jit</tt> Decorator Targeting a Single Core on the CPU\n",
    "\n",
    "Next, let us use the <tt>@jit</tt> decorator to compile the function. The first one uses the\n",
    "default target which is a single core.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example jit decorator parallel'></a>\n",
    "## 2.3 <tt>@jit</tt> Decorator Targeting Multi-Core (parallel=True)\n",
    "\n",
    "This code simply compiles for multi-core and also disables Python fallback.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import clock\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def init(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    return pos, accel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel = init(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example vectorize single'></a>\n",
    "## 2.4 <tt>@vectorize</tt> Decorator for a Single Core (default)\n",
    "\n",
    "This example rewrites the code to use the <tt>@vectorize</tt> decorator. Remember that this\n",
    "requires the code to be executed element-by-element. It really also requires that\n",
    "each function return one result. This will require some code changes to split the\n",
    "two loops so that each has its own function.\n",
    "\n",
    "Recall that we write the function as if it were a scalar. Numba takes care of\n",
    "all of the details of creating the ufunc based on the code.\n",
    "\n",
    "Another thing to note is that we have to create the arrays in the main routine. They\n",
    "can no longer be created in the functions. The <tt>@vectorize</tt> decorator only wants\n",
    "very simple element-by-element code. The precludes creating the arrays in the functions.\n",
    "\n",
    "Pay close attention to the type signature(s) for the decorator and how the functions\n",
    "are called. It is a bit counter intuitive. \n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'])\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'])\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example vectorize parallel'></a>\n",
    "## 2.5 <tt>@vectorize</tt> Decorator for Multi-Core\n",
    "\n",
    "This example simply changes the target for the <tt>@vectorize</tt> decorator to multi-core (parallel).\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'], target='parallel')\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='parallel')\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example vectorize cuda'></a>\n",
    "## 2.6 <tt>@vectorize</tt> Decorator - CUDA Target\n",
    "\n",
    "This next example also simply changes the target, but this time it is for NVIDIA GPUs.\n",
    "THis is a benefit of being able to write your function code as scalars and using Numba\n",
    "to vectorize it. You can just change the target to get either a single CPU core,\n",
    "multiple CPU cores, or an NVIDIA GPU.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def set_pos(pos):\n",
    "    return 6.5\n",
    "# end def\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def set_accel(pos, accel):\n",
    "    return 4.2*pos\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos = set_pos(pos)\n",
    "accel = set_accel(pos,accel)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Python Porting Example cuda.jit'></a>\n",
    "## 2.7 <tt>@cuda.jit</tt> Decorator for NVIDIA GPUs\n",
    "\n",
    "To use the <tt>@cuda.jit</tt> decorator, we will need to rewrite the function code. Remember\n",
    "that we have to write the explicit loops for the code now (it's not element-by-element).\n",
    "We have to change a few other things listed below.\n",
    "\n",
    "1. Everything is a Numpy array (even scalars - Numpy array of size 1)\n",
    "2. Cannot define new arrays in function and copy to host. Need to define them on host first, then copy them to device ( <tt>cuda.to_device()</tt> )\n",
    "3. Refer to scalars as <tt>array[0]</tt> (first element of array). You could create a \"local\" scalar in the functions if needed\n",
    "4. No values are returned using <tt>return</tt> function. They have to be copied back using <tt>copy_to_host</tt> method.\n",
    "5. Variables stay on GPU unless you explicitly delete them (opportunity here)\n",
    "6. Copying data between host/device can waste time - limit the amount of data transfer\n",
    "\n",
    "Notice that the timings include data transfer times. You can change this if you\n",
    "want to only time the computational portion that is on the GPU.\n",
    "\n",
    "Note: If you want to eliminate the time to compile, run the cell again without changing the code.\n",
    "You can do this several times to get an understanding of the true run time without including\n",
    "the compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def init(p_num, d_num, pos, accel):\n",
    "    for j in range(0, p_num[0]):\n",
    "        for i in range(0, d_num[0]):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num[0]):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "# end def\n",
    "\n",
    "\n",
    "# main loop\n",
    "d_num = np.zeros(1, dtype=int)\n",
    "p_num = np.zeros(1, dtype=int)\n",
    "d_num[0] = 500\n",
    "p_num[0] = 500\n",
    "pos = np.zeros( shape=(d_num[0], p_num[0]), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num[0], p_num[0]), dtype=np.float32 )\n",
    "\n",
    "start_time = perf_counter( )\n",
    "d_p_num = cuda.to_device(p_num)\n",
    "d_d_num = cuda.to_device(d_num)\n",
    "d_pos = cuda.to_device(pos)\n",
    "d_accel = cuda.to_device(accel)\n",
    "\n",
    "init(p_num, d_num, pos, accel)\n",
    "\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Functions Calling Functions'></a>\n",
    "# 3. Functions Calling Functions\n",
    "\n",
    "When running on the CPU, it's easy for a JIT compiled routine to call another JIT\n",
    "compiled routine or even just another Python routine. There is no data movement\n",
    "since there is only one pool of memory. However, calling JIT compiled GPU functions\n",
    "from other JIT compiled GPU functions is a little bit different.\n",
    "\n",
    "To explore how calling compiled functions from other compiled functions, let's\n",
    "begin with a modification of our serial Python code, adding a new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "def init_pos_accel(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "     # end for j\n",
    "    \n",
    "    vel = init_vel(p_num, d_num, pos, accel)\n",
    "\n",
    "    return pos, accel, vel\n",
    "# end init\n",
    "\n",
    "def init_vel(p_num, d_num, pos, accel):\n",
    "    vel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            vel[i,j] = pos[i,i] + accel[i,j] * 0.1*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "\n",
    "    return vel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel, vel = init_pos_accel(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(vel)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Functions Calling Functions jit'></a>\n",
    "## 3.1 Functions Calling Functions - CPUs with <tt>@jit</tt>\n",
    "\n",
    "For the a CPU example, let's compile both functions for the CPU using the\n",
    "<tt>@jit</tt> decorator. You just put the decorator before each function\n",
    "declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def init_pos_accel(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    \n",
    "    vel = init_vel(p_num, d_num, pos, accel)\n",
    "\n",
    "    return pos, accel, vel\n",
    "# end def\n",
    "\n",
    "@jit\n",
    "def init_vel(p_num, d_num, pos, accel):\n",
    "    vel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            vel[i,j] = pos[i,i] + accel[i,j] * 0.1*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "\n",
    "    return vel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel, vel = init_pos_accel(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print(vel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Functions Calling Functions jit Parallel'></a>\n",
    "## 3.2 Functions Calling Functions - <tt>@jit</tt> Targeting Multi-Core CPU\n",
    "\n",
    "For fun, let's compile the previous code but target multi-core GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def init_pos_accel(p_num, d_num):\n",
    "    pos = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    accel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "    \n",
    "    vel = init_vel(p_num, d_num, pos, accel)\n",
    "\n",
    "    return pos, accel, vel\n",
    "# end def\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def init_vel(p_num, d_num, pos, accel):\n",
    "    vel = np.zeros( shape=(d_num, p_num), dtype=np.float32 )\n",
    "    for j in range(0, p_num):\n",
    "        for i in range(0, d_num):\n",
    "            vel[i,j] = pos[i,i] + accel[i,j] * 0.1*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "\n",
    "    return vel\n",
    "# end def\n",
    "\n",
    "\n",
    "# main\n",
    "d_num = 5000\n",
    "p_num = 5000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "pos, accel, vel = init_pos_accel(p_num, d_num)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(accel)\n",
    "print(vel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Functions Calling Functions cuda.jit'></a>\n",
    "## 3.3 Functions calling Functions - GPUs with <tt>@cuda.jit</tt>\n",
    "\n",
    "A simple way for a JIT compiled GPU function to call another JIT compiled\n",
    "GPU function is to put the code for all GPU functions into\n",
    "a single function (kind of like stuffing your whole code into a function).\n",
    "This can make things complicated and may also confuse the compiler causing\n",
    "it to produce slower code.\n",
    "\n",
    "Fortunately, <tt>@cuda.jit</tt> provides a reasonable way to compile multiple functions.\n",
    "The key is to tell the functions _after_ the first one that the data is already on the\n",
    "device. This is done through an option with the decorator.\n",
    "\n",
    "\n",
    "<codeblock>\n",
    "@cuda.jit(device=True)\n",
    "</codeblock>\n",
    "\n",
    "\n",
    "Don't forget that when using the <tt>@cuda.jit</tt> decorator that you need to make\n",
    "everything a numpy array, even scalars. The code below illustrates this and also\n",
    "includes the decorators with the appropriate option.\n",
    "\n",
    "\n",
    "\n",
    "<strong>References</strong>:\n",
    "    \n",
    "1. <a href=\"https://stackoverflow.com/questions/56008378/calling-other-functions-from-within-a-cuda-jit-numba-function\">Calling other GPU Functions</a>\n",
    "\n",
    "2. <a href=\"http://numba.pydata.org/numba-doc/dev/cuda/device-functions.html\">Device Functions</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def init_all(p_num, d_num, pos, accel, vel):\n",
    "    for j in range(0, p_num[0]):\n",
    "        for i in range(0, d_num[0]):\n",
    "            pos[i,j] = 6.5\n",
    "        # end for i\n",
    "        for i in range(0, d_num[0]):\n",
    "            accel[i,j] = 4.2*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "\n",
    "    init_vel(p_num, d_num, pos, accel, vel)\n",
    "# end def\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def init_vel(p_num, d_num, pos, accel, vel):\n",
    "    for j in range(0, p_num[0]):\n",
    "        for i in range(0, d_num[0]):\n",
    "            vel[i,j] = pos[i,i] + accel[i,j] * 0.1*pos[i,j]\n",
    "        # end for i\n",
    "    # end for j\n",
    "# end def\n",
    "\n",
    "\n",
    "# main loop\n",
    "d_num = np.zeros(1, dtype=int)\n",
    "p_num = np.zeros(1, dtype=int)\n",
    "d_num[0] = 5000\n",
    "p_num[0] = 5000\n",
    "pos = np.zeros( shape=(d_num[0], p_num[0]), dtype=np.float32 )\n",
    "accel = np.zeros( shape=(d_num[0], p_num[0]), dtype=np.float32 )\n",
    "vel = np.zeros( shape=(d_num[0], p_num[0]), dtype=np.float32 ) \n",
    "\n",
    "start_time = perf_counter( )\n",
    "d_p_num = cuda.to_device(p_num)\n",
    "d_d_num = cuda.to_device(d_num)\n",
    "d_pos = cuda.to_device(pos)\n",
    "d_accel = cuda.to_device(accel)\n",
    "d_vel = cuda.to_device(vel)\n",
    "\n",
    "init_all(p_num, d_num, pos, accel, vel)\n",
    "\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print(pos)\n",
    "print(vel)\n",
    "print(accel)\n",
    "print('')\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CUPY'></a>\n",
    "## 4. CuPy\n",
    "\n",
    "As discussed in the slides, CuPy is roughly a GPU equivalent for NumPy. It covers virtually\n",
    "all of the NumPy functions but runs them on a GPU. In addition, CuPy is also starting to port\n",
    "some SciPy routine to the GPU in a new library named <tt>cupyx.scipy</tt>.\n",
    "\n",
    "The following examples illustrate how CuPy can be used in your Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CUPY SVD Data on GPU'></a>\n",
    "## 4.1 SVD Example, Leaving the Data on the GPU\n",
    "\n",
    "This example performs a Singular Value Decomposition (SVD) on a random matrix that is\n",
    "created on the GPU. Note that the results of the SVD, the <tt>u</tt>, <tt>v</tt>, and\n",
    "<tt>s</tt> matrices, are available on the GPU after the computation but in this example,\n",
    "they are not copied back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "size = 100\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = np.random.uniform(low=-1., high=1., size=(size, size)).astype(np.float32)\n",
    "u, s, v = np.linalg.svd(A)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = cp.random.uniform(low=-1., high=1., size=(size, size)).astype(cp.float32)\n",
    "u, s, v = cp.linalg.svd(A)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CUPY SVD Data back to host'></a>\n",
    "## 4.2 SVD Example - Copy Data Back to the Host\n",
    "\n",
    "This example is the same as the previous, but the <tt>u</tt> matrix is copied\n",
    "back to the host using the <tt>asnumpy</tt> method. The \"type\" of the <tt>u</tt>\n",
    "matrix on the GPU and the <tt>u</tt> matrix on the CPU are both printed so you\n",
    "can tell that one is on the device (GPU) and one is on the host (CPU). It also\n",
    "checks the difference between the reconstructed <tt>A</tt> matrix from the SVD\n",
    "components, versus the original <tt>A</tt> matrix.\n",
    "\n",
    "Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "A_cpu = np.random.uniform(low=-1., high=1., size=(64, 64)).astype(np.float32)\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "\n",
    "u_gpu, s_gpu, v_gpu = cp.linalg.svd(A_gpu)\n",
    "print(\"type(u_gpu) = \",type(u_gpu) )\n",
    "\n",
    "u_cpu = cp.asnumpy(u_gpu)\n",
    "print(\"type(u_cpu) = \",type(u_cpu) )\n",
    "\n",
    "# ----- Check answer -----\n",
    "v_cpu = cp.asnumpy(v_gpu)\n",
    "s_cpu = cp.asnumpy(s_gpu)\n",
    "\n",
    "s_cpu_test = np.diag(s_cpu)\n",
    "result = np.allclose(A_cpu, np.dot(u_cpu, np.dot(s_cpu_test, v_cpu)), atol=1e-05)\n",
    "print(\"Check result = \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CUPY Matrix Mult Data on GPU'></a>\n",
    "## 4.3 Matrix Multiplication Example - Create Data on GPU\n",
    "\n",
    "This example creates random data on the CPU or GPU and then performs the matrix\n",
    "multiplication. Note that the result of the multiplication, the <tt>C</tt> matrix,\n",
    "is available on the GPU if you need it.\n",
    "\n",
    "Notice the similarity of the two parts of the code (numpy and cupy).\n",
    "They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C = np.matmul(A,B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C = cp.matmul(A,B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CUPY Matrix Mult Data on CPU'></a>\n",
    "## 4.4 Matrix Multiplication Example - Copy Data from Host to GPU\n",
    "\n",
    "This example creates the data on the CPU and then copies it to\n",
    "the GPU. Matrix Multiplication on the CPU and GPU is timed. The\n",
    "GPU timing includes the time for the data movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4000\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C = np.matmul(A,B)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_gpu = cp.asarray(A)\n",
    "B_gpu = cp.asarray(B)\n",
    "#A = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "#B = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "C_cpu = cp.asnumpy(C_gpu)\n",
    "result = np.allclose(C, C_cpu, atol=1e-03)\n",
    "print(\"    Check result = \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DASK example'></a>\n",
    "# 5. Dask Example\n",
    "\n",
    "Dask allows you to code different tasks to solve a problem and distribute\n",
    "them to different nodes. It has been developed in coordinated manner with NumPy,\n",
    "Pandas, and Scikit-learn, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask create client'></a>\n",
    "## 5.1 First Step - Create Dask Client\n",
    "\n",
    "This example will illustrate how to compute an SVD of a matrix. We're going to\n",
    "test it on a single GPU to learn how it works. The performance won't be good\n",
    "since multiple tasks will share a single GPU.\n",
    "\n",
    "The first step in using Dask is to create a Dask \"client\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask serial SVD'></a>\n",
    "## 5.2 Compute SVD using Numpy\n",
    "\n",
    "For comparison, let's compute the Singular Value Decomposition (SVD) on\n",
    "our CPU. The SVD is one of the algorithms that can be computed using Dask\n",
    "(distributed computation). The code below is simple NumPy code to compute\n",
    "the SVD of a random matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter( )\n",
    "x = np.random.random((5000, 1000))\n",
    "\n",
    "u, s, v = np.linalg.svd(x)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for CPU SVD = %g seconds.' % (stop_time - start_time) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask parallel SVD task graph'></a>\n",
    "## 5.3 Creating the Task Graph\n",
    "\n",
    "One of the cool things about Dask is that you can use the classic NumPy functions!\n",
    "The matrix is random and is created on the first CPU.\n",
    "\n",
    "The code below illustrates how to use Dask to run the distributed SVD. Note that\n",
    "you use the NumPy function for the SVD but Dask intercepts the call so that it\n",
    "can use it's distributed routine. The function call, <tt>np.linalg.svd</tt>, actually performs\n",
    "the SVD in a distributed manner. In this example, we're just running it on a single\n",
    "node utilizing the CPUs.\n",
    "\n",
    "Dask uses a graph algorithm for computing. The code below creates the task graph\n",
    "for computing the SVD. The last command displays the task graph. It might seem a\n",
    "bit complicated, but Dask hides all that complexity from you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask\n",
    "\n",
    "x = np.random.random((5000, 1000))\n",
    "\n",
    "d = da.from_array(x, chunks=(1000, 1000))\n",
    "\n",
    "u, s, v = np.linalg.svd(d)\n",
    "\n",
    "dask.visualize(u, s, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask parallel SVD compute'></a>\n",
    "## 5.4 Bringing the Results Back to a Single Node\n",
    "\n",
    "Now let's compute the SVD!! In fact, let's compute the singular values\n",
    "(<tt>s</tt> matrix), and the two unitary matrices, <tt>u</tt> and <tt>v</tt>.\n",
    "After the computations, the firs singular value is printed as well as the\n",
    "\"shape\" of the unitary matrices.\n",
    "\n",
    "Finally, the code performs a \"check\" on the decomposition by comparing\n",
    "it to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter( )\n",
    "s_local = s.compute()\n",
    "u_local = u.compute()\n",
    "v_local = v.compute()\n",
    "stop_time = perf_counter( )\n",
    "print(\"First singular value = \",s_local[0])\n",
    "print(\"shape(u) = \", u.shape)\n",
    "print(\"shape(v) = \", v.shape)\n",
    "\n",
    "print('')\n",
    "print('Elapsed wall clock time for CPU SVD = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "# ----- Check answer -----\n",
    "s_local_test = np.diag(s_local)\n",
    "result = np.allclose(x, np.dot(u_local, np.dot(s_local_test, v_local)), atol=1e-04)\n",
    "print(\"Check result = \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed is likely to be slower than what we computed on the CPU but that's\n",
    "not the point. The point is to show you how simple it is to use Dask to use\n",
    "more resources to solve the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask parallel SVD CuPy'></a>\n",
    "## 5.5 Run SVD using Dask and CuPy - Task Graph\n",
    "\n",
    "We can do the same SVD but run it on GPUs! Dask is already GPU\n",
    "aware and can interoperate with CuPy, so we really only need to create\n",
    "the array on the GPUs (or copy to the GPUs) and tell Dask to distribute\n",
    "across the nodes, and then compute!\n",
    "\n",
    "You can run this simple example on your laptop or GPU VM if you have an\n",
    "NVIDIA GPU, but remember that you only have one GPU, so the work\n",
    "becomes serialzed and may take longer than computing across all of the\n",
    "CPU cores. But it will compute the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy\n",
    "import dask.array as da\n",
    "\n",
    "x = cupy.random.random((5000, 1000))\n",
    "\n",
    "d = da.from_array(x, chunks=(1000, 1000))\n",
    "\n",
    "u, s, v = np.linalg.svd(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask parallel SVD CuPy compute'></a>\n",
    "### 5.5.1 Compute the SVD\n",
    "The <tt>compute()</tt> method performs the computations and brings the distributed\n",
    "pieces of the computed results back to the starting node. At this point\n",
    "they are local to this node.\n",
    "\n",
    "The codebelow also \"checks\" the decomposition by comparing it to the\n",
    "original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter( )\n",
    "s_local = s.compute()\n",
    "u_local = u.compute()\n",
    "v_local = v.compute()\n",
    "stop_time = perf_counter( )\n",
    "print(\"First singular value = \",s_local[0])\n",
    "print(\"shape(u) = \", u.shape)\n",
    "print(\"shape(v) = \", v.shape)\n",
    "\n",
    "print('')\n",
    "print('Elapsed wall clock time for GPU SVD = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "# ----- Check answer -----\n",
    "s_local_test = np.diag(s_local)\n",
    "result = np.allclose(x, np.dot(u_local, np.dot(s_local_test, v_local)), atol=1e-04)\n",
    "print(\"Check result = \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dask stop client'></a>\n",
    "## 5.6 Need to Stop the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
